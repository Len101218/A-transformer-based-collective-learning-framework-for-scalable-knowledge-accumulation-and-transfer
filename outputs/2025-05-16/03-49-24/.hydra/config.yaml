project_root: /home/len1218/documents/BT/framework
setup:
  seed: 1
  setup: metaworld
  base_path: null
  save_dir: ${setup.base_path}/logs/experiment_test
  device: cuda:0
  id: sample
  description: Sample Task
  tags: null
  git:
    commit_id: null
    has_uncommitted_changes: null
    issue_id: null
  date: null
  slurm_id: null
  debug:
    should_enable: false
  metadata_path: /home/andi/Desktop/mtrl/metadata/task_embedding/roberta_small/metaworld-all.json
experiment:
  name: collective_metaworld
  mode: train_worker
  builder:
    _target_: mtrl.experiment.${experiment.name}.Experiment
  init_steps: 0
  num_train_steps: 100000
  col_sampling_freq: 10000
  col_training_samples: 800
  col_training_warmup: 8000
  train_worker: true
  use_unscaled: true
  reset_goal_state: true
  reset_goal_state_in_oder: false
  follow_scripted: false
  num_recording_steps: 6000
  only_scripted_step: 0
  scripted_porb: 0.4
  init_record_step: 2000
  num_actor_train_step: 50001
  num_critic_train_step: 10000
  num_online_train_step: 60001
  init_steps_online_tf: 8000
  follow_col_network: false
  stu_expert_steps: 40000
  actor_update_freq: 2
  distill_scripted_policy: true
  init_steps_stu: 4000
  num_student_online_trainsteps: 20000
  init_steps_stu2: 4000
  num_student_online_trainsteps2: 50000
  expert_train_step: 50000
  constant_expert_weight: 4000
  start_weight: 1.0
  use_scripted_mu: false
  evaluate_transformer: collective_network
  evaluate_episodes: 50
  evaluate_critic_rounds: 5
  recording_eval_episodes: 1
  latent_clustering_num: 1
  col_eval_freq: 1000
  eval_latent_representation: false
  evaluate_unused: false
  eval_freq: 4000
  num_eval_episodes: 10
  should_resume: true
  save:
    model:
      retain_last_n: 1
    buffer:
      should_save: true
      size_per_chunk: 20000
      num_samples_to_save: -1
      delete_replaybuffer: false
  save_dir: ${setup.save_dir}
  save_freq: 20000
  save_freq_transformer: 10000
  save_video: true
  save_video_freq: 500000
  envs_to_exclude_during_training: []
worker:
  name: worker_sac
  encoder_feature_dim: 50
  num_layers: 0
  num_filters: 0
  builder:
    _target_: mtrl.agent.sac.Agent
    actor_cfg: ${worker.actor}
    critic_cfg: ${worker.critic}
    multitask_cfg: ${worker.multitask}
    alpha_optimizer_cfg: ${worker.optimizers.alpha}
    actor_optimizer_cfg: ${worker.optimizers.actor}
    critic_optimizer_cfg: ${worker.optimizers.critic}
    discount: 0.99
    init_temperature: 0.01
    actor_update_freq: 1
    critic_tau: 0.005
    critic_target_update_freq: 2
    encoder_tau: 0.05
  actor:
    _target_: mtrl.agent.components.actor.Actor
    num_layers: 2
    hidden_dim: 256
    log_std_bounds:
    - -20
    - 2
    encoder_cfg: ${worker.encoder}
    multitask_cfg: ${worker.multitask}
  critic:
    _target_: mtrl.agent.components.critic.Critic
    hidden_dim: ${worker.actor.hidden_dim}
    num_layers: ${worker.actor.num_layers}
    encoder_cfg: ${worker.encoder}
    multitask_cfg: ${worker.multitask}
  encoder:
    type_to_select: identity
    identity:
      type: identity
      feature_dim: ${worker.encoder_feature_dim}
    feedforward:
      type: feedforward
      hidden_dim: 50
      num_layers: 2
      feature_dim: ${worker.encoder_feature_dim}
      should_tie_encoders: true
    film:
      type: film
      hidden_dim: 50
      num_layers: 2
      feature_dim: ${worker.encoder_feature_dim}
      should_tie_encoders: true
    moe:
      type: moe
      encoder_cfg:
        type: feedforward
        hidden_dim: 50
        num_layers: 2
        feature_dim: ${worker.encoder_feature_dim}
        should_tie_encoders: true
      num_experts: 9
      task_id_to_encoder_id_cfg:
        mode: cluster
        num_envs: ${env.num_envs}
        gate:
          embedding_dim: 50
          hidden_dim: 50
          num_layers: 2
          temperature: 1.0
          should_use_soft_attention: false
          topk: 2
          task_encoder_cfg:
            should_use_task_encoding: true
            should_detach_task_encoding: true
        attention:
          embedding_dim: 50
          hidden_dim: 50
          num_layers: 2
          temperature: 1.0
          should_use_soft_attention: true
          task_encoder_cfg:
            should_use_task_encoding: true
            should_detach_task-encoding: true
        cluster:
          env_name: ${env.name}
          task_description: ${env.description}
          ordered_task_list: ${env.ordered_task_list}
          mapping_cfg: ${worker.task_to_encoder_cluster}
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
        identity:
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
        ensemble:
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
    factorized_moe:
      type: fmoe
      encoder_cfg: ${worker.encoder.feedforward}
      num_factors: 2
      num_experts_per_factor:
      - 5
      - 5
    pixel:
      type: pixel
      feature_dim: ${worker.encoder_feature_dim}
      num_filters: ${worker.num_filters}
      num_layers: ${worker.num_layers}
  transition_model:
    _target_: mtrl.agent.components.transition_model.make_transition_model
    transition_cfg:
      type: ''
      feature_dim: ${worker.encoder_feature_dim}
      layer_width: 512
    multitask_cfg: ${worker.multitask}
  mask:
    num_tasks: ${env.num_envs}
    num_eval_episodes: ${experiment.num_eval_episodes}
    batch_size: ${replay_buffer.batch_size}
  multitask:
    num_envs: 1
    should_use_disentangled_alpha: true
    should_use_task_encoder: false
    should_use_multi_head_policy: false
    should_use_disjoint_policy: false
    task_encoder_cfg:
      model_cfg:
        _target_: mtrl.worker.components.task_encoder.TaskEncoder
        pretrained_embedding_cfg:
          should_use: false
          path_to_load_from: /private/home/sodhani/projects/mtrl/metadata/task_embedding/roberta_small/${env.name}.json
          ordered_task_list: ${env.ordered_task_list}
        num_embeddings: ${worker.multitask.num_envs}
        embedding_dim: 50
        hidden_dim: 50
        num_layers: 2
        output_dim: 50
      optimizer_cfg: ${worker.optimizers.actor}
      losses_to_train:
      - critic
      - transition_reward
      - decoder
      - task_encoder
    multi_head_policy_cfg:
      mask_cfg: ${worker.mask}
    actor_cfg:
      should_condition_model_on_task_info: false
      should_condition_encoder_on_task_info: true
      should_concatenate_task_info_with_encoder: true
      moe_cfg:
        mode: soft_modularization
        num_experts: 4
        should_use: false
    critic_cfg: ${worker.multitask.actor_cfg}
  gradnorm:
    alpha: 1.0
  task_to_encoder_cluster:
    mt10:
      cluster:
        action_close:
        - close
        action_default:
        - insert
        - pick and place
        - press
        - reach
        action_open:
        - open
        action_push:
        - push
        object_default:
        - button
        - door
        - peg
        - revolving joint
        object_drawer:
        - drawer
        object_goal:
        - goal
        object_puck:
        - puck
        object_window:
        - window
    mt50:
      cluster:
        action_close:
        - close
        action_default:
        - insert
        - pick and place
        - press
        - reach
        action_open:
        - open
        action_push:
        - push
        object_default:
        - button
        - door
        - peg
        - revolving joint
        object_drawer:
        - drawer
        object_goal:
        - goal
        object_puck:
        - puck
        object_window:
        - window
  optimizers:
    actor:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    alpha:
      _target_: torch.optim.Adam
      lr: 0.0001
      betas:
      - 0.9
      - 0.999
    critic:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    decoder:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
      weight_decay: 1.0e-07
    encoder:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
transformer_collective_network:
  name: transformer_collective_sac
  encoder_feature_dim: 50
  num_layers: 0
  num_filters: 0
  embedding_save_path: ${setup.save_dir}/embeddings/
  builder:
    _target_: mtrl.agent.transformer_agent.TransformerAgent
    actor_cfg: ${transformer_collective_network.actor}
    critic_cfg: ${transformer_collective_network.critic}
    transformer_encoder_cfg: ${transformer_collective_network.transformer_encoder}
    actor_optimizer_cfg: ${transformer_collective_network.optimizers.actor}
    critic_optimizer_cfg: ${transformer_collective_network.optimizers.critic}
    alpha_optimizer_cfg: ${transformer_collective_network.optimizers.alpha}
    transformer_encoder_optimizer_cfg: ${transformer_collective_network.optimizers.transformer_encoder}
    discount: 0.99
    critic_tau: 0.01
    critic_target_update_freq: 2
    actor_update_freq: 2
    use_tra_preprocessing: false
    use_state_embedding: false
    use_cls_prediction_head: true
    cluster_latent_space: false
    additional_input_state: true
    use_task_id: false
    use_zeros: false
    tra_preprocessing_dim: 8
    init_temperature: 0.1
    dropout: 0.0
    mse_loss_actor: true
  actor:
    _target_: mtrl.agent.components.actor.TransformerActor
    num_layers: 2
    hidden_dim: 300
    log_std_bounds:
    - -20
    - 2
    transformer_encoder_cfg: ${transformer_collective_network.transformer_encoder}
  critic:
    _target_: mtrl.agent.components.critic.TransformerCritic
    hidden_dim: ${transformer_collective_network.actor.hidden_dim}
    num_layers: ${transformer_collective_network.actor.num_layers}
    transformer_encoder_cfg: ${transformer_collective_network.transformer_encoder}
  transformer_encoder:
    transformer:
      _target_: mtrl.agent.components.transformer_trajectory_encoder.EncoderOnlyTransformerModel
      ntoken: 44
      d_model: 32
      nhead: 4
      dim_feedforward: 128
      nlayers: 2
      compression_dim: 0
      dropout: 0.0
      with_cls: false
      model_path: /home/andi/Desktop/mtrl/Transformer_RNN/transformer_checkpoint.pth
    representation_transformer:
      _target_: mtrl.agent.components.transformer_trajectory_encoder.RepresentationEncoderTransformer
      state_dim: 21
      action_dim: 4
      d_model: 32
      nhead: 8
      dim_feedforward: 128
      nlayers: 3
      sequence_len: 20
      dropout: 0.0
      model_path: /home/andi/Desktop/mtrl/Transformer_RNN/checkpoints/representation_cls_transformer_checkpoint.pth
    prediction_head_cls:
      _target_: mtrl.agent.components.transformer_trajectory_encoder.ClsPredictionHead
      model_path: /home/andi/Desktop/mtrl/Transformer_RNN/checkpoints/representation_cls_transformer_checkpoint.pth
      latent_dim: 6
    bnpy_model:
      _target_: mtrl.agent.components.transformer_trajectory_encoder.Bnpy_model
      bnpy_load_path: /home/andi/Desktop/mtrl/Transformer_RNN/bnpy_save/save
  optimizers:
    actor:
      _target_: torch.optim.Adam
      lr: 0.0002
      betas:
      - 0.9
      - 0.999
    critic:
      _target_: torch.optim.Adam
      lr: 0.0002
      betas:
      - 0.9
      - 0.999
    alpha:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    transformer_encoder:
      _target_: torch.optim.Adam
      lr: 1.0e-05
      betas:
      - 0.9
      - 0.999
student:
  name: student_sac
  encoder_feature_dim: 50
  num_layers: 0
  num_filters: 0
  builder:
    _target_: mtrl.agent.sac.Agent
    actor_cfg: ${student.actor}
    critic_cfg: ${student.critic}
    multitask_cfg: ${student.multitask}
    alpha_optimizer_cfg: ${student.optimizers.alpha}
    actor_optimizer_cfg: ${student.optimizers.actor}
    critic_optimizer_cfg: ${student.optimizers.critic}
    discount: 0.99
    init_temperature: 1.0
    actor_update_freq: 1
    critic_tau: 0.005
    critic_target_update_freq: 2
    encoder_tau: 0.05
  actor:
    _target_: mtrl.agent.components.actor.Actor
    num_layers: 2
    hidden_dim: 256
    log_std_bounds:
    - -20
    - 2
    encoder_cfg: ${student.encoder}
    multitask_cfg: ${student.multitask}
  critic:
    _target_: mtrl.agent.components.critic.Critic
    hidden_dim: ${student.actor.hidden_dim}
    num_layers: ${student.actor.num_layers}
    encoder_cfg: ${student.encoder}
    multitask_cfg: ${student.multitask}
  encoder:
    type_to_select: identity
    identity:
      type: identity
      feature_dim: ${student.encoder_feature_dim}
    feedforward:
      type: feedforward
      hidden_dim: 50
      num_layers: 2
      feature_dim: ${student.encoder_feature_dim}
      should_tie_encoders: true
    film:
      type: film
      hidden_dim: 50
      num_layers: 2
      feature_dim: ${student.encoder_feature_dim}
      should_tie_encoders: true
    moe:
      type: moe
      encoder_cfg:
        type: feedforward
        hidden_dim: 50
        num_layers: 2
        feature_dim: ${student.encoder_feature_dim}
        should_tie_encoders: true
      num_experts: 9
      task_id_to_encoder_id_cfg:
        mode: cluster
        num_envs: ${env.num_envs}
        gate:
          embedding_dim: 50
          hidden_dim: 50
          num_layers: 2
          temperature: 1.0
          should_use_soft_attention: false
          topk: 2
          task_encoder_cfg:
            should_use_task_encoding: true
            should_detach_task_encoding: true
        attention:
          embedding_dim: 50
          hidden_dim: 50
          num_layers: 2
          temperature: 1.0
          should_use_soft_attention: true
          task_encoder_cfg:
            should_use_task_encoding: true
            should_detach_task-encoding: true
        cluster:
          env_name: ${env.name}
          task_description: ${env.description}
          ordered_task_list: ${env.ordered_task_list}
          mapping_cfg: ${student.task_to_encoder_cluster}
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
        identity:
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
        ensemble:
          num_eval_episodes: ${experiment.num_eval_episodes}
          batch_size: ${replay_buffer.batch_size}
    factorized_moe:
      type: fmoe
      encoder_cfg: ${student.encoder.feedforward}
      num_factors: 2
      num_experts_per_factor:
      - 5
      - 5
    pixel:
      type: pixel
      feature_dim: ${student.encoder_feature_dim}
      num_filters: ${student.num_filters}
      num_layers: ${student.num_layers}
  transition_model:
    _target_: mtrl.agent.components.transition_model.make_transition_model
    transition_cfg:
      type: ''
      feature_dim: ${student.encoder_feature_dim}
      layer_width: 512
    multitask_cfg: ${student.multitask}
  mask:
    num_tasks: ${env.num_envs}
    num_eval_episodes: ${experiment.num_eval_episodes}
    batch_size: ${replay_buffer.batch_size}
  multitask:
    num_envs: ${env.num_envs}
    should_use_disentangled_alpha: false
    should_use_task_encoder: false
    should_use_multi_head_policy: false
    should_use_disjoint_policy: false
    task_encoder_cfg:
      model_cfg:
        _target_: mtrl.student.components.task_encoder.TaskEncoder
        pretrained_embedding_cfg:
          should_use: false
          path_to_load_from: /private/home/sodhani/projects/mtrl/metadata/task_embedding/roberta_small/${env.name}.json
          ordered_task_list: ${env.ordered_task_list}
        num_embeddings: ${student.multitask.num_envs}
        embedding_dim: 50
        hidden_dim: 50
        num_layers: 2
        output_dim: 50
      optimizer_cfg: ${student.optimizers.actor}
      losses_to_train:
      - critic
      - transition_reward
      - decoder
      - task_encoder
    multi_head_policy_cfg:
      mask_cfg: ${student.mask}
    actor_cfg:
      should_condition_model_on_task_info: false
      should_condition_encoder_on_task_info: true
      should_concatenate_task_info_with_encoder: true
      moe_cfg:
        mode: soft_modularization
        num_experts: 4
        should_use: false
    critic_cfg: ${student.multitask.actor_cfg}
  gradnorm:
    alpha: 1.0
  task_to_encoder_cluster:
    mt10:
      cluster:
        action_close:
        - close
        action_default:
        - insert
        - pick and place
        - press
        - reach
        action_open:
        - open
        action_push:
        - push
        object_default:
        - button
        - door
        - peg
        - revolving joint
        object_drawer:
        - drawer
        object_goal:
        - goal
        object_puck:
        - puck
        object_window:
        - window
    mt50:
      cluster:
        action_close:
        - close
        action_default:
        - insert
        - pick and place
        - press
        - reach
        action_open:
        - open
        action_push:
        - push
        object_default:
        - button
        - door
        - peg
        - revolving joint
        object_drawer:
        - drawer
        object_goal:
        - goal
        object_puck:
        - puck
        object_window:
        - window
  optimizers:
    actor:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    alpha:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    critic:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
    decoder:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
      weight_decay: 1.0e-07
    encoder:
      _target_: torch.optim.Adam
      lr: 0.0003
      betas:
      - 0.9
      - 0.999
env:
  name: metaworld-mt1
  num_envs: 1
  benchmark:
    _target_: metaworld.MT1
    env_name: reach-v2
  should_perform_reward_normalization: true
  dummy: ${env.benchmark}
  description: null
replay_buffer:
  replay_buffer:
    _target_: mtrl.replay_buffer.ReplayBuffer
    env_obs_shape: null
    action_shape: null
    capacity: 140000
    batch_size: 512
  col_replay_buffer:
    _target_: mtrl.col_replay_buffer.DistilledReplayBuffer
    env_obs_shape: null
    action_shape: null
    capacity: 2000000
    batch_size: 512
    normalize_rewards: true
  col_tmp_replay_buffer:
    _target_: mtrl.replay_buffer.ReplayBuffer
    env_obs_shape: null
    action_shape: null
    capacity: 2000000
    batch_size: 128
  transformer_col_replay_buffer:
    _target_: mtrl.transformer_replay_buffer.TransformerReplayBuffer
    env_obs_shape: null
    action_shape: null
    capacity: 2000000
    batch_size: 512
    normalize_rewards: false
    task_encoding_shape: 6
    compressed_state: true
logger:
  _target_: mtrl.logger.Logger
  logger_dir: ${setup.save_dir}
  use_tb: true
metrics:
  train:
  - - episode
    - E
    - int
    - average
  - - step
    - S
    - int
    - average
  - - duration
    - D
    - time
    - average
  - - episode_reward
    - R
    - float
    - average
  - - success
    - Su
    - float
    - average
  - - batch_reward
    - BR
    - float
    - average
  - - actor_loss
    - ALOSS
    - float
    - average
  - - critic_loss
    - CLOSS
    - float
    - average
  - - ae_loss
    - RLOSS
    - float
    - average
  - - ae_transition_loss
    - null
    - float
    - average
  - - reward_loss
    - null
    - float
    - average
  - - actor_target_entropy
    - null
    - float
    - average
  - - actor_entropy
    - null
    - float
    - average
  - - alpha_loss
    - null
    - float
    - average
  - - alpha_value
    - null
    - float
    - average
  - - contrastive_loss
    - MLOSS
    - float
    - average
  - - max_rat
    - MR
    - float
    - average
  - - env_index
    - ENV
    - str
    - constant
  - - episode_reward_env_index_
    - R_
    - float
    - average
  - - success_env_index_
    - Su_
    - float
    - average
  - - env_index_
    - ENV_
    - str
    - constant
  - - batch_reward_agent_index_
    - null
    - float
    - average
  - - critic_loss_agent_index_
    - AGENT_
    - float
    - average
  - - actor_distilled_agent_loss_agent_index_
    - null
    - float
    - average
  - - actor_loss_agent_index_
    - null
    - float
    - average
  - - actor_target_entropy_agent_index_
    - null
    - float
    - average
  - - actor_entropy_agent_index_
    - null
    - float
    - average
  - - alpha_loss_agent_index_
    - null
    - float
    - average
  - - alpha_value_agent_index_
    - null
    - float
    - average
  - - ae_loss_agent_index_
    - null
    - float
    - average
  - - mu_div
    - MUD
    - float
    - average
  - - log_std_div
    - STD
    - float
    - average
  - - kl_div
    - KLD
    - float
    - average
  - - vae_loss
    - VLOSS
    - float
    - average
  - - K_comps
    - DP_Comps
    - int
    - average
  - - kl_weight
    - KLW
    - float
    - average
  - - kl_weight_
    - KLW
    - float
    - average
  eval:
  - - episode
    - E
    - int
    - average
  - - step
    - S
    - int
    - average
  - - episode_reward
    - R
    - float
    - average
  - - env_index
    - ENV
    - str
    - constant
  - - success
    - Su
    - float
    - average
  - - episode_reward_env_index_
    - R_
    - float
    - average
  - - success_env_index_
    - Su_
    - float
    - average
  - - env_index_
    - ENV_
    - str
    - constant
  - - batch_reward_agent_index_
    - AGENT_
    - float
    - average
  col_train:
  - - episode
    - E
    - int
    - average
  - - step
    - S
    - int
    - average
  - - duration
    - D
    - time
    - average
  - - episode_reward
    - R
    - float
    - average
  - - success
    - Su
    - float
    - average
  - - batch_reward
    - BR
    - float
    - average
  - - actor_loss
    - ALOSS
    - float
    - average
  - - critic_loss
    - CLOSS
    - float
    - average
  - - ae_loss
    - RLOSS
    - float
    - average
  - - ae_transition_loss
    - null
    - float
    - average
  - - reward_loss
    - null
    - float
    - average
  - - actor_target_entropy
    - null
    - float
    - average
  - - actor_entropy
    - null
    - float
    - average
  - - alpha_loss
    - null
    - float
    - average
  - - alpha_value
    - null
    - float
    - average
  - - contrastive_loss
    - MLOSS
    - float
    - average
  - - max_rat
    - MR
    - float
    - average
  - - env_index
    - ENV
    - str
    - constant
  - - episode_reward_env_index_
    - R_
    - float
    - average
  - - success_env_index_
    - Su_
    - float
    - average
  - - env_index_
    - ENV_
    - str
    - constant
  - - batch_reward_agent_index_
    - null
    - float
    - average
  - - critic_loss_agent_index_
    - AGENT_
    - float
    - average
  - - actor_distilled_agent_loss_agent_index_
    - null
    - float
    - average
  - - actor_loss_agent_index_
    - null
    - float
    - average
  - - actor_target_entropy_agent_index_
    - null
    - float
    - average
  - - actor_entropy_agent_index_
    - null
    - float
    - average
  - - alpha_loss_agent_index_
    - null
    - float
    - average
  - - alpha_value_agent_index_
    - null
    - float
    - average
  - - ae_loss_agent_index_
    - null
    - float
    - average
  - - mu_div
    - MUD
    - float
    - average
  - - log_std_div
    - STD
    - float
    - average
  - - kl_div
    - KLD
    - float
    - average
  - - vae_loss
    - VLOSS
    - float
    - average
  - - K_comps
    - DP_Comps
    - int
    - average
  col_eval:
  - - episode
    - E
    - int
    - average
  - - step
    - S
    - int
    - average
  - - episode_reward
    - R
    - float
    - average
  - - env_index
    - ENV
    - str
    - constant
  - - success
    - Su
    - float
    - average
  - - episode_reward_env_index_
    - R_
    - float
    - average
  - - success_env_index_
    - Su_
    - float
    - average
  - - env_index_
    - ENV_
    - str
    - constant
  - - batch_reward_agent_index_
    - AGENT_
    - float
    - average
  - - critic_loss
    - CLOSS
    - float
    - average
logbook:
  _target_: ml_logger.logbook.make_config
  write_to_console: false
  logger_dir: ${setup.save_dir}
  create_multiple_log_files: false
